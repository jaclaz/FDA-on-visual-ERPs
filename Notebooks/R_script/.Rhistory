#------------------------------------#
### PROBABILITY INTEGRAL TRANSFORM ###
#------------------------------------#
# Se U ~ U[0,1] -> X = -log(1-U)/lambda ~ Exp(lambda)
# Campioniamo N realizzazioni da una distribuzione uniforme in [0,1]
# e mostriamo che l'istogramma di x = -log(1-u)/lambda approssima
# una distribuzione exponenziale di parametro lambda.
N = 500
u = runif(N)
lambda = 3
x = -log(1-u)/lambda
hist(x, prob=T)
yy = seq(0,100,by=0.1)
fexp = lambda*exp(-lambda*yy)
lines(yy, fexp, lwd=2, type='l', col='red')
f<-function(x){
return(dbeta(x, 3, 6))
}
xx = seq(0,1,by=0.01)
yy = f(xx)
plot(xx, yy, xlab='x', ylab='f(x)', main='Distribuzione Beta(3,6)', type='l', col='red')
# Considerando come candidate density g(x) densità di un'uniforme U[0,1]
g<-function(x){
return(dunif(x))
}
# abbiamo che f(x)/g(x) <= M con M = 2.6
M = 2.6
abline(h=M, lty=2, col='blue')
# Accept-reject loop
N = 5000 # numero di realizzazioni che vogliamo
x <- NULL
for(i in 1:N){
accept = FALSE
while(accept==FALSE){
u = runif(1) # realizzazione da U[0,1]
y = runif(1) # realizzazione dalla candidate density  g (U[0,1])
if(u<=(f(y)/(M*g(y)))){
x = c(x, y) # se vale la consdizione x = y, altrimenti continuo
accept = TRUE
}
}
}
# Plottiamo l'istogramma delle x ottenute e la densità Beta(3,6)
hist(x, prob=T)
lines(xx, yy, xlab='x', type='l', col='red')
abline(h=M, lty=2, col='blue')
#---------------------#
#### NUOVI COMANDI ####
#---------------------#
help( rbinom ) # Density ( d ), distribution function ( p ), quantile function ( q ) and random generation ( r )
# for the binomial distribution with parameters size and prob.
help( rexp ) # Density ( d ), distribution function ( p ), quantile function ( q ) and random generation ( r )
p = seq( 0, 1, .1 ) # Considero diversi valori assunti da p
n = 20     # Suppongo di avre confezioni da n = 20 bulloni
N = 500     # Simulo N = 500 realizzazioni della variabile binomiale
dev.new()
###########################################################
#### 2 - Q-Q PLOT E VERIFICA DELLA NORMALITA' DEI DATI ####
###########################################################
graphics.off()
rm(list = ls())
ls()
n = 0
ls()
rm(ls())
rm(list = ls())
###########################################################
#### 2 - Q-Q PLOT E VERIFICA DELLA NORMALITA' DEI DATI ####
###########################################################
graphics.off()
#---------------------#
#### COMANDI UTILI ####
#---------------------#
help( rnorm ) # Density ( d ), distribution function ( p ), quantile function ( q ) and random generation ( r )
# for the normal distribution with mean equal to mean and standard deviation equal to sd.
help( qqnorm ) # produces a normal QQ plot
mu = 0
sigma = 1
N = 10000 # Simulo N realizzazioni della variabile gaussiana
# Campionamento tramite la funzione 'rnorm'
dati.sim = rnorm( N, mean = mu, sd = sigma )
# Facciamo l'istogramma per valutare l'andamento della distribuzione dei dati
# campionati rispetto al modello teorico
dev.new()
hist( dati.sim, prob = T, main = 'Istogramma dati simulati da Normale' )
x = seq( min( dati.sim )-.1, max( dati.sim ) + .1, length = 100 )
y = dnorm( x, mean = mu, sd = sigma )
lines( x, y, col = 'red', lwd = 2 )
# Costruiamo in automatico del q-q plot con la funzione 'qqnorm'
dev.new()
# Costruiamo in automatico del q-q plot con la funzione 'qqnorm'
dev.new()
qqnorm( dati.sim )
qqline( dati.sim, col = 'red', lwd = 2 )
# Campioniamo ora da una gaussiana non standard
mu2 = 5
sigma2 = 2.5
dati.sim2 = rnorm( N, mean = mu2, sd = sigma2 )
# Disegnamo il q-q plot
dev.new()
qqnorm( dati.sim2 )
qqline( dati.sim2, col = 'red', lwd = 2 )
# Mi aspetto che la retta non coincida con la bisettrice
abline( 0, 1, lwd = 2, col = 'green', lty = 2 )
dati.stand = ( dati.sim2 - mean( dati.sim2 ) )/sd( dati.sim2 )
dev.new()
qqnorm( dati.stand )
qqline( dati.stand, col = 'red', lwd = 2 )
abline( 0, 1, col = 'green', lty = 3, lwd = 2 )
#-------------------#
#### ESERCIZIO 2 ####
#-------------------#
graphics.off()
dev.new()
qqnorm( dati.stand )
qqline( dati.stand, col = 'red', lwd = 2 )
abline( 0, 1, col = 'green', lty = 3, lwd = 2 )
#-------------------#
#### ESERCIZIO 2 ####
#-------------------#
graphics.off()
# Osservazioni iid da una normale standard
z = rnorm( 1000 )
# Osservazioni iid da una U( -2, 2 )
u = runif( 1000, -2, 2 )
# Osservazioni iid da una Exp( 1/3 )
g = rexp( 1000, 1/3 )
# Osservazioni iid da una t-student con 2 gradi di libertà
t = rt( 1000, df = 2 )
x11()
par( mfrow = c( 2, 4 ) )
# Grafico densità
# Normale
curve( dnorm( x ), -5, 5, ylab = 'densità', main = 'Normale', col = 2)
# Uniforme
curve( dnorm( x ), -5, 5, ylab = 'densità', main = 'Uniforme' )
curve( dunif( x, -2, 2 ), -5, 5, add = TRUE, col = 2 )
legend( 1, 0.4, c( "N(0,1)", "U(-2,2)" ), col = c( 1, 2 ), lty = 1, cex = 1, x.intersp = 0.5, seg.len=0.6)
# Esponenziale
curve( dnorm( x ), -5, 5, ylab = 'densità', main = 'Esponenziale' )
curve( dexp( x, 1/3 ), -5, 5, add = TRUE, col = 2 )
legend( 1, 0.4, c( "N(0,1)", "Exp(1/3)" ), col = c( 1, 2 ), lty = 1, cex = 1, x.intersp = 0.5, seg.len=0.6)
# T-Student
curve( dnorm( x ), -5, 5, ylab = 'densità', main = 't di Student' )
curve( dt( x, df = 1 ), -5, 5, add = TRUE, col = 2 )
legend(1, 0.4, c( "N(0,1)", "t-stud(2)" ), col = c( 1, 2 ), lty = 1, cex = 1, x.intersp = 0.5, seg.len=0.6)
# QQ-plot
# Normale
qqnorm( z )
qqline( z, col = 'red' )
# Uniforme
qqnorm( u )
qqline( u, col = 'red' )
# Esponenziale
qqnorm( g )
qqline( g, col = 'red' )
# T-Student
qqnorm( t )
qqline( t, col = 'red' )
#############################################
#### 3 - LEGGE DEI GRANDI NUMERI ( LGN ) ####
#############################################
rm( list = ls() )
graphics.off()
# Lancio una moneta: campionamento da una Bernoulli
n = 500 # Numero di lanci che voglio simulare
p = 0.7 # Probabilità di successo ( successo = esce testa )
dati.ber = rbinom( n, size = 1, prob = p )
# 'rbinom' genera n osservazioni di una Binom( 1, p ), quindi di una Bern( p ).
dev.new()
barplot( table( dati.ber ), main = 'Barplot frequenze assolute lanci moneta', ylab = 'Frequenze assolute', names.arg = c( 'Croce', 'Testa' ) )
somma.ber = cumsum( dati.ber )
# 'cumsum' calcola la somma cumulata
# Per calcolare la media campionaria dividiamo la somma cumulata per
# il numero di lanci corrispondente
n.lanci = 1:n
media.camp = somma.ber/n.lanci
# Visualizziamo la media campionaria al variare del numero di lanci
dev.new()
plot( n.lanci, media.camp, type = 'l', lwd = 2,
main = 'Simulazione: la legge dei grandi numeri', xlab = 'numero lanci',
ylab = 'proporzione di successi', ylim = c( 0, 1 ) )
# Aggiungiamo una linea orizzontale in corrispondenza del valore teorico
# a cui dovrebbe tendere la media campionaria delle v.a. di Bernoulli
abline( h = p, col = 'red', lwd = 2 )
#################################################
#### 4 - TEOREMA CENTRALE DEL LIMITE ( TCL ) ####
#################################################
rm( list = ls() )
graphics.off()
n = c( 5, 50, 500, 5000 ) # Numero di variabili del campione che voglio simulare
p = 0.7 # Probabilità di successo associata alla Bernoulli
N = 500 # Numero di realizzazioni per ciascuna variabile
dati = NULL
for( i in n ){
# Genero N realizzazioni di una Binom( n, p )
dati.bin = rbinom( N, size = i, prob = p )
dati = cbind( dati, dati.bin )
# Metto i dati simulati in un'unica matrice Nx4, in cui ogni colonna contiene
# le N realizzazioni della Binom( n, p ), con n che assume i 4 diversi valori considerati.
}
dev.new()
par( mfrow = c( 2, 2 ) )
x = seq( min( dati [ , 1 ] ), max( dati [ , 1 ] ), .01 )
media = n [ 1 ] * p
devstand = sqrt( n [ 1 ] * p * ( 1-p ) )
density = dnorm( x, mean = media, sd = devstand )
# Istogramma:
hist( dati [ , 1 ] , prob = TRUE, main = 'TCL applicato a somma di Bernoulli, n = 5',
xlab = 'somma di n bernoulli',
ylim = c( 0, max( max( density ), max( hist( dati [ , 1 ] , plot = F )$density ) ) ),
breaks = seq( min( dati [ , 1 ] )-.5, max( dati [ , 1 ] ) + .5, by = 1 ) )
# Densità normale sovrapposta
lines( x, density, col = 'red', lwd = 2 )
x = seq( min( dati [ , 2 ] ), max( dati [ , 2 ] ), .01 )
media = n [ 2 ] * p
devstand = sqrt( n [ 2 ] * p * ( 1-p ) )
density = dnorm( x, mean = media, sd = devstand )
hist( dati [ , 2 ] , prob = TRUE, main = 'TCL applicato a somma di Bernoulli, n = 50',
xlab = 'somma di n bernoulli',
ylim = c( 0, max( max( density ), max( hist( dati [ , 2 ] , plot = F )$density ) ) ) )
#breaks = seq( min( dati [ , 2 ] )-.5, max( dati [ , 2 ] ) + .5, by = 1 ) )
lines( x, density, col = 'red', lwd = 2 )
x = seq( min( dati [ , 3 ] ), max( dati [ , 3 ] ), .01 )
media = n [ 3 ] * p
devstand = sqrt( n [ 3 ] * p * ( 1-p ) )
density = dnorm( x, mean = media, sd = devstand )
hist( dati [ , 3 ] , prob = TRUE, main = 'TCL applicato a somma di Bernoulli, n = 500',
xlab = 'somma di n bernoulli',
ylim = c( 0, max( max( density ), max( hist( dati [ , 3 ] , plot = F )$density ) ) ) )
lines( x, density, col = 'red', lwd = 2 )
x = seq( min( dati [ , 4 ] ), max( dati [ , 4 ] ), .01 )
media = n [ 4 ] * p
devstand = sqrt( n [ 4 ] * p * ( 1-p ) )
density = dnorm( x, mean = media, sd = devstand )
hist( dati [ , 4 ] , prob = TRUE, main = 'TCL applicato a somma di Bernoulli, n = 5000',
xlab = 'somma di n bernoulli',
ylim = c( 0, max( max( density ), max( hist( dati [ , 4 ] , plot = F )$density ) ) ) )
lines( x, density, col = 'red', lwd = 2 )
dev.new()
par( mfrow = c( 2, 2 ) )
qqnorm( dati [ , 1 ] , main = 'TCL applicato a somma di Bernoulli, n = 5' )
qqline( dati [ , 1 ] , lwd = 2, col = 'red' )
qqnorm( dati [ , 2 ] , main = 'TCL applicato a somma di Bernoulli, n = 50' )
qqline( dati [ , 2 ] , lwd = 2, col = 'red' )
qqnorm( dati [ , 3 ] , main = 'TCL applicato a somma di Bernoulli, n = 500' )
qqline( dati [ , 3 ] , lwd = 2, col = 'red' )
qqnorm( dati [ , 4 ] , main = 'TCL applicato a somma di Bernoulli, n = 5000' )
qqline( dati [ , 4 ] , lwd = 2, col = 'red' )
######################################
#### 5 - INTEGRAZIONE MONTE-CARLO ####
######################################
rm( list = ls() )
graphics.off()
set.seed(123)
n = 100
x = runif(n)    # Genero gli x_i
y = 4/(1+x^2)   # Calcolo y_i = h(x_i)
h_n = sum(y)/n
h_n
Int_esatto = pi # 4*arctg(1)
errore = abs(h_n - Int_esatto)
errore
# Al variare di n:
set.seed(123)
n.prove = seq(1,5000,by=10)
H = NULL
for(n in n.prove){
x = runif(n)
y = 4/(1+x^2)
H = c(H, sum(y)/n)
}
x11()
plot(n.prove, H, type='l', ylab=expression(h[n]), xlab='n')
abline(h=pi, col='red', lty=2, lwd=2)
mu = 0
sigma = 1
N = 10000 # Simulo N realizzazioni della variabile gaussiana
# Campionamento tramite la funzione 'rnorm'
dati.sim = rnorm( N, mean = mu, sd = sigma )
# Facciamo l'istogramma per valutare l'andamento della distribuzione dei dati
# campionati rispetto al modello teorico
dev.new()
hist( dati.sim, prob = T, main = 'Istogramma dati simulati da Normale' )
x = seq( min( dati.sim )-.1, max( dati.sim ) + .1, length = 100 )
y = dnorm( x, mean = mu, sd = sigma )
lines( x, y, col = 'red', lwd = 2 )
set.seed(123)
n = 100
x = runif(n)    # Genero gli x_i
y = 4/(1+x^2)   # Calcolo y_i = h(x_i)
h_n = sum(y)/n
h_n
Int_esatto = pi # 4*arctg(1)
errore = abs(h_n - Int_esatto)
errore
# Cosa succede quando campioniamo dati indipendenti tra loro con diversi valori di n?
set.seed(123)
n.prove = seq(1,5000,by=1)
H = NULL
for(n in n.prove){
x = runif(n)
y = 4/(1+x^2)
H = c(H, sum(y)/n)
}
x11()
plot(n.prove, H, type='l', ylab=expression(h[n]), xlab='n')
abline(h=pi, col='red', lty=2, lwd=2)
library( car )
library( ellipse )
library( faraway )
library( leaps )
library(MASS)
library( GGally)
library(rgl)
library( qpcR )
install.packages('faraway')
library( faraway )
library( car )
library( ellipse )
library( faraway )
library( leaps )
library(MASS)
library( GGally)
library(rgl)
library( car )
library( ellipse )
library( faraway )
library( leaps )
library(MASS)
library( GGally)
library(rgl)
# import data
data(savings)
View(savings)
# Dimensions
dim(savings)
# Overview of the first rows
head(savings)
#Look at the main statistics for each variable:
summary(savings)
#If missing values were present, 'summary' function would have informed us. To check it directly:
# observe that there are no missing values
sum(is.na(savings))  # na.omit(savings)
print(sapply(savings,function(x) any(is.na(x))))
#Finally we get the data type of each column:
# check the type of each column (integer, double, character, ...)
print(sapply(savings, typeof))
# or
str(savings)
pairs(savings, pch=16)
pairs(savings[ , c('sr', 'pop15', 'pop75', 'dpi', 'ddpi')], pch = 16)
ggpairs(data = savings, title ="Relationships between predictors & response",
lower = list(continuous=wrap("points", alpha = 0.5, size=0.1)))
g = lm( sr ~ pop15 + pop75 + dpi + ddpi, data = savings )
#g = lm( sr ~ ., savings )
summary( g )
gs = summary( g )
names(g) # this gives you the attributes of the linear model object
g$call # linear model forumla
g$coefficients #beta_hat
View(g)
g$fitted.values # estimated 'sr' for each observation
#We could also compute directly the fitted values of the dependent variable:
X = model.matrix(g)
y_hat_man = X %*% g$coefficients #beta_hat
g$residuals # residuals
g$rank # the numeric rank of the fitted linear model (number of covariates + 1)
#Calculate Variance-Covariance Matrix for a Fitted Model Object
vcov( g )
SS_tot = sum( ( savings$sr-mean( savings$sr ) )^2 )
SS_res = sum( g$res^2 )
p = g$rank # p = 5
n = dim(savings)[1] # n = 50
f_test = ( ( SS_tot - SS_res )/(p-1) )/( SS_res/(n-p) )
## p-value (right-hand area of f_test)
1 - pf( f_test, p - 1, n - p )
X = model.matrix( g )
sigma2 = (summary( g )$sigma)^2
#manually
sigma2 = sum( ( savings$sr - g$fitted.values )^2 ) / ( n -p )
se_beta_1 = summary( g )$coef[ 2, 2 ]
#manually
se_beta_1 = sqrt( sigma2 * diag( solve( t( X ) %*% X ) )[2] )
T.0 = abs( ( g$coefficients[ 2 ] - 0 )/ se_beta_1 )
2*( 1-pt( T.0, n-p ) )
alpha = 0.05
t_alpha2 = qt( 1-alpha/2, n-p )
beta_hat_pop75 = g$coefficients[3]
se_beta_hat_pop75 = summary( g )[[4]][3,2]
IC_pop75 = c( beta_hat_pop75 - t_alpha2 * se_beta_hat_pop75,
beta_hat_pop75 + t_alpha2 * se_beta_hat_pop75 )
IC_pop75
#We observe that the IC includes $0$, so there is no evidence for rejecting $H_0: \beta_2 = 0$, at the $5% level.
# Indeed, this parameter was not significant even in the previous section (p-value $12.5\%$).
summary(g)$coef[3,4]
alpha = 0.05
t_alpha2 = qt( 1-alpha/2, n-p )
beta_hat_ddpi = g$coefficients[5]
se_beta_hat_ddpi = summary( g )[[4]][5,2]
IC_ddpi = c( beta_hat_ddpi - t_alpha2 * se_beta_hat_ddpi,
beta_hat_ddpi + t_alpha2 * se_beta_hat_ddpi )
IC_ddpi
summary(g)$coef[5,4]
#help( ellipse )
plot( ellipse( g, c( 2, 3 ) ), type = "l", xlim = c( -1, 0 ) )
# add the origin (we test that the coeff are (0,0)) and the point of the estimates:
points( 0, 0 )
# add also the center of the ellipse, that is, the estimated couple of coefficients
points( g$coef[ 2 ] , g$coef[ 3 ] , pch = 18 )
#__REMARK__ It is important to stress that this Confidence Region is different from the one obtained by the cartesian product of
#the two Confidence Intervals, $IC_{(1-\alpha)}(\beta_1)$ X $IC_{(1-\alpha)}(\beta_2)$.
#The cartesian product of the two Confidence Intervals is represented by the four dashed lines.
beta_hat_pop15 = g$coefficients[2]
se_beta_hat_pop15 = summary( g )[[4]][2,2]
IC_pop15 = c( beta_hat_pop15 - t_alpha2 * se_beta_hat_pop15,
beta_hat_pop15 + t_alpha2 * se_beta_hat_pop15 )
IC_pop15
plot( ellipse( g, c( 2, 3 ) ), type = "l", xlim = c( -1, 0 ) )
points( 0, 0 )
points( g$coef[ 2 ] , g$coef[ 3 ] , pch = 18 )
#new part
abline( v = c( IC_pop15[1], IC_pop15[2] ), lty = 2 )
abline( h = c( IC_pop75[1], IC_pop75[2] ), lty = 2 )
plot( ellipse( g, c( 2, 3 ) ), type = "l", xlim = c( -1, 0 ) )
points( 0, 0 )
points( g$coef[ 2 ] , g$coef[ 3 ] , pch = 18 )
abline( v = c( IC_pop15[1], IC_pop15[2] ), lty = 2 )
abline( h = c( IC_pop75[1], IC_pop75[2] ), lty = 2 )
#new part
points( -0.22, 0.7, col = "red", lwd = 2 )
points( -0.71, 0, col = "blue", lwd = 2 )
cor( savings$pop15, savings$pop75 )
plot( g$fit, g$res, xlab = "Fitted", ylab = "Residuals",
main = "Residuals vs Fitted Values", pch = 16 )
abline( h = 0, lwd = 2, lty = 2, col = 'red' )  # variance seems uniform across the fitted values
#Alternatively, we can plot
plot(g, which=1 )
# QQ plot
qqnorm( g$res, ylab = "Raw Residuals", pch = 16 )
qqline( g$res )
# Shapiro-Wilk normality test
shapiro.test( g$res )
# other useful tools...
hist( g$res, 10, probability = TRUE, col = 'lavender', main = 'residuals'  )
boxplot( g$res, main = "Boxplot of savings residuals", pch = 16, col = 'lavender' )
summary(g)
g2 = lm(sr ~ pop15 + pop75 + ddpi, data = savings)
summary(g2)
g3 = lm(sr ~ pop15 +  ddpi, data = savings)
summary(g3)
plot3d(savings$pop15, savings$ddpi, savings$sr, xlab='pop15', ylab='ddpi', zlab='sr')
planes3d( -0.21638, 0.44283,-1, 15.59958, col = 'blue', alpha = 0.6) # ax + by + cz +d = 0
rm(list=ls())
graphics.off()
library( car )
library( ellipse )
library( faraway )
library( leaps )
library( MASS )
library( GGally)
library( rgl )
library(RColorBrewer)
library(ggplot2)
pluto=read.table("scores.csv",sep=",",header=TRUE)
attach(pluto)
setwd("C:/Users/Asus/OneDrive/Documenti/GitHub/FDA-on-visual-ERPs/Notebooks")
pluto=read.table("scores.csv",sep=",",header=TRUE)
attach(pluto)
pluto$Category=factor(pluto$Category, labels=c('Animal', 'Body Part', 'Vehicle', 'Tool', 'Food'))
pluto$X.1=factor(pluto$X.1, labels=c('1','2','3','4'))
G <- ggplot(data = pluto,
aes(x = X.1, y = X0))
x11()
G +  scale_fill_brewer(palette = 'Set2') +
geom_boxplot(aes(fill = Category), show.legend = T) +
#facet_wrap(~struc, nrow = 3) +
theme_minimal(base_size =  20) +
theme(legend.position = 'bottom') +
labs(x = 'Principal Component Function', y = 'Score') +
ggtitle('Scores of the Functional Components')
ggsave('scores_fpca', plot = last_plot(), dpi = 300)
ggsave('scores_fpca.png', plot = last_plot(), dpi = 300)
ggsave('scores_fpca.png', plot = last_plot(), dpi = 300)
